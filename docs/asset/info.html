<p>
    Asset services provide access to media assets - previews and video in appropriate formats.  Asset services are used in conjunction with list transactions to enumerate and identify assets.
</p>
<p>
    Assets are identified by a tuple of timestamp, cameraid, quality, and format.
</p>
<ul>
    <li>Timestamp: Eagle Eye timestamps have the format YYYYMMDDhhmmss.xxx and are always specifined in GMT time.  In most contexts, special tokens can also be used to specify relative times - “now” is the current time, a value starting with + or - is an offset from the current time.</li>
    <li>CameraID: Cameras are identified by a 8 character hexadecimal string, representing a unique 32 bit id associated with a specific camera.  Note CameraID are not necessarily linked to specific hardware devices to allow device upgrade and replacement without disruption of history storage.</li>
    <li>Quality: (low,med,high) Images and video may have multiple quality levels, each representing the same base asset.  Video can be transcoded between quality levels on demand (at some point) to support reduced bandwidth for mobile devices.  Normally cameras will capture at medium or high quality.  For video, low quality is targeted at around 100kbps, medium quality is under 500kbps, and high quality is around 1mbps.  Additional quality levels will be supported in time.</li>
    <li>Format: Images are always returned as JPEG images.  Video can currently returned as either FLV format (playback in browsers via Flash), MP4 (download and export format), and m3u/mpegts (HttpStreaming for iOS and newer android devices).</li>
</ul>
<h3>Request Image</h3>
<p>
The image request model provides an efficient mechanism for accessing image sequences for several usage models.  Image requests can be done directly using the next/after/prev virtual model. This returns images before or after specified timestamps.  Alternatively, the timestamp and event information can be fetched through the “list” interface (to get events for history) and “poll” interface to track new images as they  become available in real-time.  The following cookbooks provides typical usage models for various events.
</p>
<ul>
    <li>low bandwidth video playback:  The preview stream is a sequential set of jpeg images.  If played back in order, low resolution video is accomplished.
        <ul>
            <li>The simplest implementatation is to fetch “next” with a timestamp of “now” (e. g. /asset/next/image.jpeg?t=now;c=12345678;a=pre) - waiting for the subsequent image after the current time.  Each time an image is returned, a new request should be made. If the downstream bandwidth is very low, the image fetch will automatically slow down (because delivery of image A happens after image B has been received, so the next call will fetch image C, skipping display of image B entirely).  This approach works well for tracking a single image stream.  As a tip, the first request should be done as a “prev” request to make sure an image is displayed, before the sequential next requests.  The downside of this model is it requires a dedicated socket for each image stream being played.  Many browsers have a limited pool of open sockets.</li>
            <li>A more efficient mechanism for tracking multiple image streams is to use the “poll” interface. The Poll interface will provide the timestamp of the next image available for a set of camera, which can then be fetched via the /asset/asset call.  Since the poll request supports multiple cameras in a single request, it requires only a single socket for any number of cameras.  The client application should implement a “fair” algorithm across the returned timestamps to address low bandwidth situations (that is, make sure every image stream gets updated before you fetch a new image for the same stream).  The best model for this is
                <ul>
                    <li>receive update notifications for all cameras being tracked via a single sequential poll session</li>
                    <li>for each camera, keep track of the “lastest” image notification, replacing the last one even if it has not been fetched yet.</li>
                    <li>with a limited pool of “requests”, do a fair rotation between all cameras, fetching only the most recent image for each, and skipping the fetch if the image is already loading.</li>
                </ul>
                This algorithm will provide smooth frame rate degradation across any number of cameras, whether the performance bottleneck is client CPU or bandwidth.
            </li>
         </ul>
    </li>
    <li>Random access image discovery.  The preview and thumb image streams can provide a visual navigation tool for accessing recorded video.  The typical implementation requires a map from a timestamp to the “best” image for that timestamp.  To implement this approach, the client should use the “after” and “prev” requests with the timestamp of the user playhead.  Both calls provide header data for x-ee-timestamp, x-ee-next, and x-ee-prev which identify the current and subsequent images in both directions when it can be easily determined.  The usage paradigm for this should be
        <ul>
            <li>On navigation event (large jump), determine the timestamp of the user playhead and do an “/asset/prev” call to get the appropriate image (hmm, this should really be a best,  figure out how ugly that would be).  Store the x-ee-timestamp, x-ee-next and x-ee-prev values for the image.</li>
            <li>As the user moves the playhead, if the time change is within the next/prev halfway bounds, no new request is required.  when the use moves outside of the time range, do an image fetch with the new timestamp.</li>
        </ul>
    </li>
    <li>Thumbnail navigation.  The system provides a “thumbnail” image for each event which is intended to provide a small representation of the event.  The easiest mechanism to get a thumbnail for an event is to do an /asset/after/image.jpeg?a=thumb... image request with the starting timestamp of the event.</li>
</ul>